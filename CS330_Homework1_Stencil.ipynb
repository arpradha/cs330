{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvkoC8rAYBE7"
   },
   "source": [
    "\n",
    "##Setup\n",
    "\n",
    "You will need to make a copy of this Colab notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "-Q4lGYC_E6QQ",
    "outputId": "17e3e1dd-cf1b-4a91-96a1-6b7612949b2a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "# Need to download the Omniglot dataset -- DON'T MODIFY THIS CELL\n",
    "if not os.path.isdir('./omniglot_resized'):\n",
    "    gdd.download_file_from_google_drive(file_id='1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI',\n",
    "                                        dest_path='./omniglot_resized.zip',\n",
    "                                        unzip=True)\n",
    "    \n",
    "assert os.path.isdir('./omniglot_resized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ucYsULp9HUJy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "\n",
    "def get_images(paths, labels, nb_samples=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Takes a set of character folders and labels and returns paths to image files\n",
    "    paired with labels.\n",
    "    Args:\n",
    "        paths: A list of character folders\n",
    "        labels: List or numpy array of same length as paths\n",
    "        nb_samples: Number of images to retrieve per character\n",
    "    Returns:\n",
    "        List of (label, image_path) tuples\n",
    "    \"\"\"\n",
    "    if nb_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, nb_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    images_labels = [(i, os.path.join(path, image))\n",
    "                     for i, path in zip(labels, paths)\n",
    "                     for image in sampler(os.listdir(path))]\n",
    "    if shuffle:\n",
    "        random.shuffle(images_labels)\n",
    "    return images_labels\n",
    "\n",
    "\n",
    "def image_file_to_array(filename, dim_input):\n",
    "    \"\"\"\n",
    "    Takes an image path and returns numpy array\n",
    "    Args:\n",
    "        filename: Image filename\n",
    "        dim_input: Flattened shape of image\n",
    "    Returns:\n",
    "        1 channel image\n",
    "    \"\"\"\n",
    "    import imageio\n",
    "    image = imageio.imread(filename)  # misc.imread(filename)\n",
    "    image = image.reshape([dim_input])\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = 1.0 - image\n",
    "    return image\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, num_samples_per_class, config={}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of classes for classification (K-way)\n",
    "            num_samples_per_class: num samples to generate per class in one batch\n",
    "            batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        data_folder = config.get('data_folder', './omniglot_resized')\n",
    "        self.img_size = config.get('img_size', (28, 28))\n",
    "\n",
    "        self.dim_input = np.prod(self.img_size)\n",
    "        self.dim_output = self.num_classes\n",
    "\n",
    "        character_folders = [os.path.join(data_folder, family, character)\n",
    "                             for family in os.listdir(data_folder)\n",
    "                             if os.path.isdir(os.path.join(data_folder, family))\n",
    "                             for character in os.listdir(os.path.join(data_folder, family))\n",
    "                             if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "\n",
    "        random.seed(1)\n",
    "        random.shuffle(character_folders)\n",
    "        num_val = 100\n",
    "        num_train = 1100\n",
    "        self.metatrain_character_folders = character_folders[: num_train]\n",
    "        self.metaval_character_folders = character_folders[\n",
    "            num_train:num_train + num_val]\n",
    "        self.metatest_character_folders = character_folders[\n",
    "            num_train + num_val:]\n",
    "\n",
    "    def sample_batch(self, batch_type, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a batch for training, validation, or testing\n",
    "        Args:\n",
    "            batch_type: train/val/test\n",
    "        Returns:\n",
    "            A a tuple of (1) Image batch and (2) Label batch where\n",
    "            image batch has shape [B, K, N, 784] and label batch has shape [B, K, N, N]\n",
    "            where B is batch size, K is number of samples per class, N is number of classes\n",
    "        \"\"\"\n",
    "        if batch_type == \"train\":\n",
    "            folders = self.metatrain_character_folders\n",
    "        elif batch_type == \"val\":\n",
    "            folders = self.metaval_character_folders\n",
    "        else:\n",
    "            folders = self.metatest_character_folders\n",
    "\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        all_image_batches = np.zeros(shape=(batch_size, self.num_samples_per_class, self.num_classes, self.dim_input), dtype=np.float32)\n",
    "        all_label_batches = np.zeros(shape=(batch_size, self.num_samples_per_class, self.num_classes, self.num_classes), dtype=np.float32)\n",
    "        \n",
    "\n",
    "        for batch_id in range(batch_size):\n",
    "            tmp = np.random.randint(0, self.num_classes)\n",
    "            paths = np.random.choice(folders, self.num_classes)\n",
    "            one_hot_labels = np.identity(self.num_classes)\n",
    "            if batch_type == \"test\":\n",
    "                np.random.shuffle(one_hot_labels)\n",
    "                \n",
    "            images_labels = get_images(paths, range(self.num_classes), nb_samples=self.num_samples_per_class, shuffle=False)\n",
    "            \n",
    "            sam_per_class = {}\n",
    "            for i in range(self.num_classes):\n",
    "                sam_per_class[i] = 0\n",
    "\n",
    "            for idx, (label, fname) in enumerate(images_labels):                  \n",
    "                img = image_file_to_array(fname, self.dim_input)\n",
    "                all_image_batches[batch_id, sam_per_class[label], label, :] = img\n",
    "                all_label_batches[batch_id, sam_per_class[label], label, :] = one_hot_labels[:, label]\n",
    "                sam_per_class.update({label:sam_per_class[label]+1})\n",
    "\n",
    "            \n",
    "        #############################\n",
    "\n",
    "        return all_image_batches.astype(np.float32), all_label_batches.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 4\n",
    "num_samples = 2\n",
    "data_generator = DataGenerator(num_classes, num_samples + 1)\n",
    "i, l = data_generator.sample_batch('test', 100)\n",
    "i.shape, l.shape\n",
    "\n",
    "l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1yvqTOqTHVA9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class MANN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes, samples_per_class):\n",
    "        super(MANN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.layer1 = tf.keras.layers.LSTM(128, return_sequences=True)\n",
    "        self.layer2 = tf.keras.layers.LSTM(num_classes, return_sequences=True)\n",
    "\n",
    "    def call(self, input_images, input_labels):\n",
    "        \"\"\"\n",
    "        MANN\n",
    "        Args:\n",
    "            input_images: [B, K+1, N, 784] flattened images\n",
    "            labels: [B, K+1, N, N] ground truth labels\n",
    "        Returns:\n",
    "            [B, K+1, N, N] predictions\n",
    "        \"\"\"\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        #print('anu')\n",
    "        B, K, N, D = input_images.shape\n",
    "        images = tf.reshape(input_images, (B, K*N, D))\n",
    "        \n",
    "        # filling the last N examples with zeros\n",
    "        labels = tf.concat((input_labels[:,:-1], tf.zeros_like(input_labels[:, -1:])), axis=1)\n",
    "        \n",
    "        # reshaping to match with images\n",
    "        labels = tf.reshape(labels, (B, K*N, N))\n",
    "\n",
    "        tmp = self.layer1(tf.concat((images, labels), axis=-1))\n",
    "        tmp = self.layer2(tmp)\n",
    "        out = tf.reshape(tmp, (B, K, N, N))\n",
    "        #############################\n",
    "        return out\n",
    "\n",
    "    def loss_function(self, preds, labels):\n",
    "        \"\"\"\n",
    "        Computes MANN loss\n",
    "        Args:\n",
    "            preds: [B, K+1, N, N] network output\n",
    "            labels: [B, K+1, N, N] labels\n",
    "        Returns:\n",
    "            scalar loss\n",
    "        \"\"\"\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        loss = tf.reduce_sum(tf.keras.losses.categorical_crossentropy(y_true=labels[:, -1, :, :], y_pred=preds[:, -1, :, :], from_logits=True))\n",
    "        #############################\n",
    "        return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels, model, optim, eval=False):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, labels)\n",
    "        loss = model.loss_function(predictions, labels)\n",
    "    if not eval:\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return predictions, loss\n",
    "\n",
    "\n",
    "def main(num_classes=5, num_samples=1, meta_batch_size=16, random_seed=1234):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)\n",
    "\n",
    "    data_generator = DataGenerator(num_classes, num_samples + 1)\n",
    "\n",
    "    o = MANN(num_classes, num_samples + 1)\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    result = dict()\n",
    "    \n",
    "    for step in range(25000):\n",
    "        i, l = data_generator.sample_batch('train', meta_batch_size)\n",
    "        _, ls = train_step(i, l, o, optim)\n",
    "\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print(\"*\" * 5 + \"Iter \" + str(step + 1) + \"*\" * 5)\n",
    "            i, l = data_generator.sample_batch('test', 100)\n",
    "            pred, tls = train_step(i, l, o, optim, eval=True)\n",
    "            print(\"Train Loss:\", ls.numpy(), \"Test Loss:\", tls.numpy())\n",
    "            pred = tf.reshape(pred, [-1, num_samples + 1, num_classes, num_classes])\n",
    "            pred = tf.math.argmax(pred[:, -1, :, :], axis=2)\n",
    "            l = tf.math.argmax(l[:, -1, :, :], axis=2)\n",
    "            print(\"Test Accuracy\", tf.reduce_mean(tf.cast(tf.math.equal(pred, l), tf.float32)).numpy())\n",
    "            result[step] = {'train_loss':ls.numpy(), 'test_loss':tls.numpy(), 'test_acc':tf.reduce_mean(tf.cast(tf.math.equal(pred, l), tf.float32)).numpy()}\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "si10vH_0SH_y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Iter 100*****\n",
      "Train Loss: 5.90996 Test Loss: 215.6136\n",
      "Test Accuracy 0.43\n",
      "*****Iter 200*****\n",
      "Train Loss: 5.3239756 Test Loss: 208.02322\n",
      "Test Accuracy 0.49\n",
      "*****Iter 300*****\n",
      "Train Loss: 5.259412 Test Loss: 219.9459\n",
      "Test Accuracy 0.465\n",
      "*****Iter 400*****\n",
      "Train Loss: 5.240785 Test Loss: 183.5468\n",
      "Test Accuracy 0.56\n",
      "*****Iter 500*****\n",
      "Train Loss: 5.2426844 Test Loss: 209.20026\n",
      "Test Accuracy 0.495\n",
      "*****Iter 600*****\n",
      "Train Loss: 5.226169 Test Loss: 183.2774\n",
      "Test Accuracy 0.57\n",
      "*****Iter 700*****\n",
      "Train Loss: 5.2211866 Test Loss: 204.39302\n",
      "Test Accuracy 0.51\n",
      "*****Iter 800*****\n",
      "Train Loss: 5.2181864 Test Loss: 183.43204\n",
      "Test Accuracy 0.57\n",
      "*****Iter 900*****\n",
      "Train Loss: 5.2152643 Test Loss: 204.48862\n",
      "Test Accuracy 0.51\n",
      "*****Iter 1000*****\n",
      "Train Loss: 5.2140813 Test Loss: 211.08575\n",
      "Test Accuracy 0.49\n",
      "*****Iter 1100*****\n",
      "Train Loss: 5.2137294 Test Loss: 210.38808\n",
      "Test Accuracy 0.49\n",
      "*****Iter 1200*****\n",
      "Train Loss: 5.2209496 Test Loss: 186.74167\n",
      "Test Accuracy 0.55\n",
      "*****Iter 1300*****\n",
      "Train Loss: 5.212445 Test Loss: 214.25414\n",
      "Test Accuracy 0.48\n",
      "*****Iter 1400*****\n",
      "Train Loss: 5.21066 Test Loss: 204.55455\n",
      "Test Accuracy 0.51\n",
      "*****Iter 1500*****\n",
      "Train Loss: 5.2103877 Test Loss: 232.69162\n",
      "Test Accuracy 0.43\n",
      "*****Iter 1600*****\n",
      "Train Loss: 5.2097096 Test Loss: 215.13191\n",
      "Test Accuracy 0.48\n",
      "*****Iter 1700*****\n",
      "Train Loss: 5.2091494 Test Loss: 204.58527\n",
      "Test Accuracy 0.51\n",
      "*****Iter 1800*****\n",
      "Train Loss: 5.210604 Test Loss: 213.52623\n",
      "Test Accuracy 0.485\n",
      "*****Iter 1900*****\n",
      "Train Loss: 5.208383 Test Loss: 215.16248\n",
      "Test Accuracy 0.48\n",
      "*****Iter 2000*****\n",
      "Train Loss: 5.209553 Test Loss: 215.17755\n",
      "Test Accuracy 0.48\n",
      "*****Iter 2100*****\n",
      "Train Loss: 5.208368 Test Loss: 219.91698\n",
      "Test Accuracy 0.465\n",
      "*****Iter 2200*****\n",
      "Train Loss: 5.2077923 Test Loss: 218.28868\n",
      "Test Accuracy 0.47\n",
      "*****Iter 2300*****\n",
      "Train Loss: 5.2081375 Test Loss: 225.58308\n",
      "Test Accuracy 0.45\n",
      "*****Iter 2400*****\n",
      "Train Loss: 5.207449 Test Loss: 208.15341\n",
      "Test Accuracy 0.5\n",
      "*****Iter 2500*****\n",
      "Train Loss: 5.2092414 Test Loss: 207.17422\n",
      "Test Accuracy 0.495\n",
      "*****Iter 2600*****\n",
      "Train Loss: 5.20769 Test Loss: 177.20288\n",
      "Test Accuracy 0.585\n",
      "*****Iter 2700*****\n",
      "Train Loss: 5.2069755 Test Loss: 207.19772\n",
      "Test Accuracy 0.5\n",
      "*****Iter 2800*****\n",
      "Train Loss: 5.2071958 Test Loss: 236.26585\n",
      "Test Accuracy 0.42\n",
      "*****Iter 2900*****\n",
      "Train Loss: 5.2073193 Test Loss: 223.82892\n",
      "Test Accuracy 0.455\n",
      "*****Iter 3000*****\n",
      "Train Loss: 5.2067213 Test Loss: 220.29684\n",
      "Test Accuracy 0.465\n",
      "*****Iter 3100*****\n",
      "Train Loss: 5.2071657 Test Loss: 222.228\n",
      "Test Accuracy 0.46\n",
      "*****Iter 3200*****\n",
      "Train Loss: 5.2065496 Test Loss: 211.68678\n",
      "Test Accuracy 0.49\n",
      "*****Iter 3300*****\n",
      "Train Loss: 5.2064486 Test Loss: 197.64217\n",
      "Test Accuracy 0.53\n",
      "*****Iter 3400*****\n",
      "Train Loss: 5.2065125 Test Loss: 215.20958\n",
      "Test Accuracy 0.48\n",
      "*****Iter 3500*****\n",
      "Train Loss: 5.206272 Test Loss: 225.74649\n",
      "Test Accuracy 0.45\n",
      "*****Iter 3600*****\n",
      "Train Loss: 5.206212 Test Loss: 180.07372\n",
      "Test Accuracy 0.58\n",
      "*****Iter 3700*****\n",
      "Train Loss: 5.206259 Test Loss: 225.73978\n",
      "Test Accuracy 0.45\n",
      "*****Iter 3800*****\n",
      "Train Loss: 5.2066274 Test Loss: 215.20956\n",
      "Test Accuracy 0.48\n",
      "*****Iter 3900*****\n",
      "Train Loss: 5.20613 Test Loss: 208.18216\n",
      "Test Accuracy 0.5\n",
      "*****Iter 4000*****\n",
      "Train Loss: 5.2060966 Test Loss: 222.23892\n",
      "Test Accuracy 0.46\n",
      "*****Iter 4100*****\n",
      "Train Loss: 5.206063 Test Loss: 197.28522\n",
      "Test Accuracy 0.53\n",
      "*****Iter 4200*****\n",
      "Train Loss: 5.206131 Test Loss: 243.18161\n",
      "Test Accuracy 0.4\n",
      "*****Iter 4300*****\n",
      "Train Loss: 5.2059674 Test Loss: 204.67621\n",
      "Test Accuracy 0.51\n",
      "*****Iter 4400*****\n",
      "Train Loss: 5.205931 Test Loss: 197.63524\n",
      "Test Accuracy 0.53\n",
      "*****Iter 4500*****\n",
      "Train Loss: 5.205998 Test Loss: 215.21465\n",
      "Test Accuracy 0.48\n",
      "*****Iter 4600*****\n",
      "Train Loss: 5.205822 Test Loss: 229.26831\n",
      "Test Accuracy 0.44\n",
      "*****Iter 4700*****\n",
      "Train Loss: 5.2058153 Test Loss: 236.29112\n",
      "Test Accuracy 0.42\n",
      "*****Iter 4800*****\n",
      "Train Loss: 5.2058024 Test Loss: 208.18953\n",
      "Test Accuracy 0.5\n",
      "*****Iter 4900*****\n",
      "Train Loss: 5.2057605 Test Loss: 211.70488\n",
      "Test Accuracy 0.49\n",
      "*****Iter 5000*****\n",
      "Train Loss: 5.206188 Test Loss: 208.19221\n",
      "Test Accuracy 0.5\n",
      "*****Iter 5100*****\n",
      "Train Loss: 5.2057633 Test Loss: 218.73117\n",
      "Test Accuracy 0.47\n",
      "*****Iter 5200*****\n",
      "Train Loss: 5.205704 Test Loss: 225.75182\n",
      "Test Accuracy 0.45\n",
      "*****Iter 5300*****\n",
      "Train Loss: 5.2059665 Test Loss: 204.6609\n",
      "Test Accuracy 0.51\n",
      "*****Iter 5400*****\n",
      "Train Loss: 5.2056527 Test Loss: 204.6791\n",
      "Test Accuracy 0.51\n",
      "*****Iter 5500*****\n",
      "Train Loss: 5.205971 Test Loss: 190.61911\n",
      "Test Accuracy 0.55\n",
      "*****Iter 5600*****\n",
      "Train Loss: 5.205624 Test Loss: 194.1363\n",
      "Test Accuracy 0.54\n",
      "*****Iter 5700*****\n",
      "Train Loss: 5.20574 Test Loss: 222.23956\n",
      "Test Accuracy 0.46\n",
      "*****Iter 5800*****\n",
      "Train Loss: 5.2056 Test Loss: 225.75957\n",
      "Test Accuracy 0.45\n",
      "*****Iter 5900*****\n",
      "Train Loss: 5.205681 Test Loss: 225.76016\n",
      "Test Accuracy 0.45\n",
      "*****Iter 6000*****\n",
      "Train Loss: 5.205581 Test Loss: 197.6557\n",
      "Test Accuracy 0.53\n",
      "*****Iter 6100*****\n",
      "Train Loss: 5.2060366 Test Loss: 197.65088\n",
      "Test Accuracy 0.53\n",
      "*****Iter 6200*****\n",
      "Train Loss: 5.205566 Test Loss: 187.10945\n",
      "Test Accuracy 0.56\n",
      "*****Iter 6300*****\n",
      "Train Loss: 5.205537 Test Loss: 187.11559\n",
      "Test Accuracy 0.56\n",
      "*****Iter 6400*****\n",
      "Train Loss: 5.205864 Test Loss: 173.06258\n",
      "Test Accuracy 0.6\n",
      "*****Iter 6500*****\n",
      "Train Loss: 5.2055206 Test Loss: 194.14235\n",
      "Test Accuracy 0.54\n",
      "*****Iter 6600*****\n",
      "Train Loss: 5.205859 Test Loss: 211.70782\n",
      "Test Accuracy 0.49\n",
      "*****Iter 6700*****\n",
      "Train Loss: 5.205511 Test Loss: 218.73274\n",
      "Test Accuracy 0.47\n",
      "*****Iter 6800*****\n",
      "Train Loss: 5.2057524 Test Loss: 176.57343\n",
      "Test Accuracy 0.59\n",
      "*****Iter 6900*****\n",
      "Train Loss: 5.20549 Test Loss: 190.62189\n",
      "Test Accuracy 0.55\n",
      "*****Iter 7000*****\n",
      "Train Loss: 5.205491 Test Loss: 197.65442\n",
      "Test Accuracy 0.53\n",
      "*****Iter 7100*****\n",
      "Train Loss: 5.2054977 Test Loss: 204.68309\n",
      "Test Accuracy 0.51\n",
      "*****Iter 7200*****\n",
      "Train Loss: 5.205638 Test Loss: 187.11116\n",
      "Test Accuracy 0.56\n",
      "*****Iter 7300*****\n",
      "Train Loss: 5.2054973 Test Loss: 215.22275\n",
      "Test Accuracy 0.48\n",
      "*****Iter 7400*****\n",
      "Train Loss: 5.2055655 Test Loss: 236.30232\n",
      "Test Accuracy 0.42\n",
      "*****Iter 7500*****\n",
      "Train Loss: 5.2054605 Test Loss: 220.64659\n",
      "Test Accuracy 0.465\n",
      "*****Iter 7600*****\n",
      "Train Loss: 5.205467 Test Loss: 215.21121\n",
      "Test Accuracy 0.48\n",
      "*****Iter 7700*****\n",
      "Train Loss: 5.205776 Test Loss: 197.6573\n",
      "Test Accuracy 0.53\n",
      "*****Iter 7800*****\n",
      "Train Loss: 5.2054696 Test Loss: 195.75381\n",
      "Test Accuracy 0.535\n",
      "*****Iter 7900*****\n",
      "Train Loss: 5.2056694 Test Loss: 183.60405\n",
      "Test Accuracy 0.57\n",
      "*****Iter 8000*****\n",
      "Train Loss: 5.20544 Test Loss: 204.5425\n",
      "Test Accuracy 0.51\n",
      "*****Iter 8100*****\n",
      "Train Loss: 5.2054353 Test Loss: 222.25023\n",
      "Test Accuracy 0.46\n",
      "*****Iter 8200*****\n",
      "Train Loss: 5.2055306 Test Loss: 222.24738\n",
      "Test Accuracy 0.46\n",
      "*****Iter 8300*****\n",
      "Train Loss: 5.2054253 Test Loss: 246.84018\n",
      "Test Accuracy 0.39\n",
      "*****Iter 8400*****\n",
      "Train Loss: 5.205427 Test Loss: 201.17085\n",
      "Test Accuracy 0.52\n",
      "*****Iter 8500*****\n",
      "Train Loss: 5.205548 Test Loss: 176.56955\n",
      "Test Accuracy 0.59\n",
      "*****Iter 8600*****\n",
      "Train Loss: 5.2054214 Test Loss: 218.73714\n",
      "Test Accuracy 0.47\n",
      "*****Iter 8700*****\n",
      "Train Loss: 5.205415 Test Loss: 232.78308\n",
      "Test Accuracy 0.43\n",
      "*****Iter 8800*****\n",
      "Train Loss: 5.205411 Test Loss: 229.27219\n",
      "Test Accuracy 0.44\n",
      "*****Iter 8900*****\n",
      "Train Loss: 5.205411 Test Loss: 218.73764\n",
      "Test Accuracy 0.47\n",
      "*****Iter 9000*****\n",
      "Train Loss: 5.2054076 Test Loss: 194.14445\n",
      "Test Accuracy 0.54\n",
      "*****Iter 9100*****\n",
      "Train Loss: 5.205407 Test Loss: 232.78807\n",
      "Test Accuracy 0.43\n",
      "*****Iter 9200*****\n",
      "Train Loss: 5.2054014 Test Loss: 176.57565\n",
      "Test Accuracy 0.59\n",
      "*****Iter 9300*****\n",
      "Train Loss: 5.2053995 Test Loss: 204.6843\n",
      "Test Accuracy 0.51\n",
      "*****Iter 9400*****\n",
      "Train Loss: 5.2053995 Test Loss: 187.1171\n",
      "Test Accuracy 0.56\n",
      "*****Iter 9500*****\n",
      "Train Loss: 5.205395 Test Loss: 208.1967\n",
      "Test Accuracy 0.5\n",
      "*****Iter 9600*****\n",
      "Train Loss: 5.2055435 Test Loss: 232.78352\n",
      "Test Accuracy 0.43\n",
      "*****Iter 9700*****\n",
      "Train Loss: 5.205392 Test Loss: 190.62955\n",
      "Test Accuracy 0.55\n",
      "*****Iter 9800*****\n",
      "Train Loss: 5.2054005 Test Loss: 225.76192\n",
      "Test Accuracy 0.45\n",
      "*****Iter 9900*****\n",
      "Train Loss: 5.205389 Test Loss: 208.19746\n",
      "Test Accuracy 0.5\n",
      "*****Iter 10000*****\n",
      "Train Loss: 5.2055473 Test Loss: 187.11469\n",
      "Test Accuracy 0.56\n",
      "*****Iter 10100*****\n",
      "Train Loss: 5.205387 Test Loss: 180.09166\n",
      "Test Accuracy 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Iter 10200*****\n",
      "Train Loss: 5.205426 Test Loss: 222.25107\n",
      "Test Accuracy 0.46\n",
      "*****Iter 10300*****\n",
      "Train Loss: 5.205391 Test Loss: 204.68483\n",
      "Test Accuracy 0.51\n",
      "*****Iter 10400*****\n",
      "Train Loss: 5.2053823 Test Loss: 201.17139\n",
      "Test Accuracy 0.52\n",
      "*****Iter 10500*****\n",
      "Train Loss: 5.205382 Test Loss: 234.38223\n",
      "Test Accuracy 0.425\n",
      "*****Iter 10600*****\n",
      "Train Loss: 5.205389 Test Loss: 197.65631\n",
      "Test Accuracy 0.53\n",
      "*****Iter 10700*****\n",
      "Train Loss: 5.205385 Test Loss: 190.62997\n",
      "Test Accuracy 0.55\n",
      "*****Iter 10800*****\n",
      "Train Loss: 5.2053795 Test Loss: 201.17151\n",
      "Test Accuracy 0.52\n",
      "*****Iter 10900*****\n",
      "Train Loss: 5.2053814 Test Loss: 197.65771\n",
      "Test Accuracy 0.53\n",
      "*****Iter 11000*****\n",
      "Train Loss: 5.2053776 Test Loss: 222.24962\n",
      "Test Accuracy 0.46\n",
      "*****Iter 11100*****\n",
      "Train Loss: 5.2053776 Test Loss: 236.30232\n",
      "Test Accuracy 0.42\n",
      "*****Iter 11200*****\n",
      "Train Loss: 5.2053766 Test Loss: 201.17154\n",
      "Test Accuracy 0.52\n",
      "*****Iter 11300*****\n",
      "Train Loss: 5.2054834 Test Loss: 218.73836\n",
      "Test Accuracy 0.47\n",
      "*****Iter 11400*****\n",
      "Train Loss: 5.2053747 Test Loss: 201.17162\n",
      "Test Accuracy 0.52\n",
      "*****Iter 11500*****\n",
      "Train Loss: 5.205377 Test Loss: 197.65732\n",
      "Test Accuracy 0.53\n",
      "*****Iter 11600*****\n",
      "Train Loss: 5.2053766 Test Loss: 209.78761\n",
      "Test Accuracy 0.495\n",
      "*****Iter 11700*****\n",
      "Train Loss: 5.205372 Test Loss: 194.14468\n",
      "Test Accuracy 0.54\n",
      "*****Iter 11800*****\n",
      "Train Loss: 5.2053733 Test Loss: 232.79099\n",
      "Test Accuracy 0.43\n",
      "*****Iter 11900*****\n",
      "Train Loss: 5.205433 Test Loss: 208.19826\n",
      "Test Accuracy 0.5\n",
      "*****Iter 12000*****\n",
      "Train Loss: 5.205382 Test Loss: 201.16978\n",
      "Test Accuracy 0.52\n",
      "*****Iter 12100*****\n",
      "Train Loss: 5.2053714 Test Loss: 232.7912\n",
      "Test Accuracy 0.43\n",
      "*****Iter 12200*****\n",
      "Train Loss: 5.20537 Test Loss: 211.71121\n",
      "Test Accuracy 0.49\n",
      "*****Iter 12300*****\n",
      "Train Loss: 5.205513 Test Loss: 201.1711\n",
      "Test Accuracy 0.52\n",
      "*****Iter 12400*****\n",
      "Train Loss: 5.205392 Test Loss: 169.55023\n",
      "Test Accuracy 0.61\n",
      "*****Iter 12500*****\n",
      "Train Loss: 5.2053695 Test Loss: 225.76454\n",
      "Test Accuracy 0.45\n",
      "*****Iter 12600*****\n",
      "Train Loss: 5.205368 Test Loss: 208.19829\n",
      "Test Accuracy 0.5\n",
      "*****Iter 12700*****\n",
      "Train Loss: 5.2053685 Test Loss: 201.17085\n",
      "Test Accuracy 0.52\n",
      "*****Iter 12800*****\n",
      "Train Loss: 5.205368 Test Loss: 229.0446\n",
      "Test Accuracy 0.44\n",
      "*****Iter 12900*****\n",
      "Train Loss: 5.2053685 Test Loss: 218.73831\n",
      "Test Accuracy 0.47\n",
      "*****Iter 13000*****\n",
      "Train Loss: 5.205368 Test Loss: 232.77722\n",
      "Test Accuracy 0.43\n",
      "*****Iter 13100*****\n",
      "Train Loss: 5.205408 Test Loss: 236.3018\n",
      "Test Accuracy 0.42\n",
      "*****Iter 13200*****\n",
      "Train Loss: 5.205368 Test Loss: 229.27122\n",
      "Test Accuracy 0.44\n",
      "*****Iter 13300*****\n",
      "Train Loss: 5.2054043 Test Loss: 208.18633\n",
      "Test Accuracy 0.5\n",
      "*****Iter 13400*****\n",
      "Train Loss: 5.2054 Test Loss: 204.68466\n",
      "Test Accuracy 0.51\n",
      "*****Iter 13500*****\n",
      "Train Loss: 5.205389 Test Loss: 180.09045\n",
      "Test Accuracy 0.58\n",
      "*****Iter 13600*****\n",
      "Train Loss: 5.2053857 Test Loss: 208.1978\n",
      "Test Accuracy 0.5\n",
      "*****Iter 13700*****\n",
      "Train Loss: 5.2053766 Test Loss: 221.15036\n",
      "Test Accuracy 0.465\n",
      "*****Iter 13800*****\n",
      "Train Loss: 5.2053747 Test Loss: 243.33118\n",
      "Test Accuracy 0.4\n",
      "*****Iter 13900*****\n",
      "Train Loss: 5.2053747 Test Loss: 218.73755\n",
      "Test Accuracy 0.47\n",
      "*****Iter 14000*****\n",
      "Train Loss: 5.2053733 Test Loss: 185.52084\n",
      "Test Accuracy 0.565\n",
      "*****Iter 14100*****\n",
      "Train Loss: 5.205372 Test Loss: 218.73793\n",
      "Test Accuracy 0.47\n",
      "*****Iter 14200*****\n",
      "Train Loss: 5.2053995 Test Loss: 208.19814\n",
      "Test Accuracy 0.5\n",
      "*****Iter 14300*****\n",
      "Train Loss: 5.205371 Test Loss: 204.68367\n",
      "Test Accuracy 0.51\n",
      "*****Iter 14400*****\n",
      "Train Loss: 5.205411 Test Loss: 215.2249\n",
      "Test Accuracy 0.48\n",
      "*****Iter 14500*****\n",
      "Train Loss: 5.205371 Test Loss: 232.79132\n",
      "Test Accuracy 0.43\n",
      "*****Iter 14600*****\n",
      "Train Loss: 5.2053695 Test Loss: 204.68489\n",
      "Test Accuracy 0.51\n",
      "*****Iter 14700*****\n",
      "Train Loss: 5.205387 Test Loss: 215.22495\n",
      "Test Accuracy 0.48\n",
      "*****Iter 14800*****\n",
      "Train Loss: 5.205369 Test Loss: 243.33134\n",
      "Test Accuracy 0.4\n",
      "*****Iter 14900*****\n",
      "Train Loss: 5.2053685 Test Loss: 211.70996\n",
      "Test Accuracy 0.49\n",
      "*****Iter 15000*****\n",
      "Train Loss: 5.205368 Test Loss: 190.63188\n",
      "Test Accuracy 0.55\n",
      "*****Iter 15100*****\n",
      "Train Loss: 5.205368 Test Loss: 176.57867\n",
      "Test Accuracy 0.59\n",
      "*****Iter 15200*****\n",
      "Train Loss: 5.2053957 Test Loss: 210.1169\n",
      "Test Accuracy 0.495\n",
      "*****Iter 15300*****\n",
      "Train Loss: 5.2053785 Test Loss: 225.76447\n",
      "Test Accuracy 0.45\n",
      "*****Iter 15400*****\n",
      "Train Loss: 5.205367 Test Loss: 236.30472\n",
      "Test Accuracy 0.42\n",
      "*****Iter 15500*****\n",
      "Train Loss: 5.2053666 Test Loss: 199.5733\n",
      "Test Accuracy 0.525\n",
      "*****Iter 15600*****\n",
      "Train Loss: 5.2053666 Test Loss: 194.14517\n",
      "Test Accuracy 0.54\n",
      "*****Iter 15700*****\n",
      "Train Loss: 5.205366 Test Loss: 211.71167\n",
      "Test Accuracy 0.49\n",
      "*****Iter 15800*****\n",
      "Train Loss: 5.205366 Test Loss: 187.11856\n",
      "Test Accuracy 0.56\n",
      "*****Iter 15900*****\n",
      "Train Loss: 5.205366 Test Loss: 218.7383\n",
      "Test Accuracy 0.47\n",
      "*****Iter 16000*****\n",
      "Train Loss: 5.205366 Test Loss: 196.13873\n",
      "Test Accuracy 0.535\n",
      "*****Iter 16100*****\n",
      "Train Loss: 5.205366 Test Loss: 176.57869\n",
      "Test Accuracy 0.59\n",
      "*****Iter 16200*****\n",
      "Train Loss: 5.2053676 Test Loss: 208.19836\n",
      "Test Accuracy 0.5\n",
      "*****Iter 16300*****\n",
      "Train Loss: 5.205365 Test Loss: 229.27762\n",
      "Test Accuracy 0.44\n",
      "*****Iter 16400*****\n",
      "Train Loss: 5.205365 Test Loss: 201.17084\n",
      "Test Accuracy 0.52\n",
      "*****Iter 16500*****\n",
      "Train Loss: 5.205365 Test Loss: 171.46718\n",
      "Test Accuracy 0.605\n",
      "*****Iter 16600*****\n",
      "Train Loss: 5.205365 Test Loss: 225.76486\n",
      "Test Accuracy 0.45\n",
      "*****Iter 16700*****\n",
      "Train Loss: 5.205365 Test Loss: 210.11769\n",
      "Test Accuracy 0.495\n",
      "*****Iter 16800*****\n",
      "Train Loss: 5.2053814 Test Loss: 215.22499\n",
      "Test Accuracy 0.48\n",
      "*****Iter 16900*****\n",
      "Train Loss: 5.205366 Test Loss: 222.25148\n",
      "Test Accuracy 0.46\n",
      "*****Iter 17000*****\n",
      "Train Loss: 5.2053647 Test Loss: 239.8179\n",
      "Test Accuracy 0.41\n",
      "*****Iter 17100*****\n",
      "Train Loss: 5.205365 Test Loss: 215.225\n",
      "Test Accuracy 0.48\n",
      "*****Iter 17200*****\n",
      "Train Loss: 5.205367 Test Loss: 211.71146\n",
      "Test Accuracy 0.49\n",
      "*****Iter 17300*****\n",
      "Train Loss: 5.205365 Test Loss: 197.65851\n",
      "Test Accuracy 0.53\n",
      "*****Iter 17400*****\n",
      "Train Loss: 5.2053647 Test Loss: 239.81807\n",
      "Test Accuracy 0.41\n",
      "*****Iter 17500*****\n",
      "Train Loss: 5.205365 Test Loss: 204.68483\n",
      "Test Accuracy 0.51\n",
      "*****Iter 17600*****\n",
      "Train Loss: 5.2053647 Test Loss: 210.11877\n",
      "Test Accuracy 0.495\n",
      "*****Iter 17700*****\n",
      "Train Loss: 5.2053657 Test Loss: 222.2516\n",
      "Test Accuracy 0.46\n",
      "*****Iter 17800*****\n",
      "Train Loss: 5.2053647 Test Loss: 169.55212\n",
      "Test Accuracy 0.61\n",
      "*****Iter 17900*****\n",
      "Train Loss: 5.2053647 Test Loss: 190.6314\n",
      "Test Accuracy 0.55\n",
      "*****Iter 18000*****\n",
      "Train Loss: 5.2053647 Test Loss: 211.71173\n",
      "Test Accuracy 0.49\n",
      "*****Iter 18100*****\n",
      "Train Loss: 5.205364 Test Loss: 201.17181\n",
      "Test Accuracy 0.52\n",
      "*****Iter 18200*****\n",
      "Train Loss: 5.205369 Test Loss: 211.7117\n",
      "Test Accuracy 0.49\n",
      "*****Iter 18300*****\n",
      "Train Loss: 5.205364 Test Loss: 194.1451\n",
      "Test Accuracy 0.54\n",
      "*****Iter 18400*****\n",
      "Train Loss: 5.205364 Test Loss: 194.14519\n",
      "Test Accuracy 0.54\n",
      "*****Iter 18500*****\n",
      "Train Loss: 5.205364 Test Loss: 211.71152\n",
      "Test Accuracy 0.49\n",
      "*****Iter 18600*****\n",
      "Train Loss: 5.2053638 Test Loss: 206.60388\n",
      "Test Accuracy 0.505\n",
      "*****Iter 18700*****\n",
      "Train Loss: 5.2053666 Test Loss: 215.225\n",
      "Test Accuracy 0.48\n",
      "*****Iter 18800*****\n",
      "Train Loss: 5.2053638 Test Loss: 225.76488\n",
      "Test Accuracy 0.45\n",
      "*****Iter 18900*****\n",
      "Train Loss: 5.2053633 Test Loss: 218.73828\n",
      "Test Accuracy 0.47\n",
      "*****Iter 19000*****\n",
      "Train Loss: 5.2053633 Test Loss: 197.65839\n",
      "Test Accuracy 0.53\n",
      "*****Iter 19100*****\n",
      "Train Loss: 5.2053633 Test Loss: 208.19838\n",
      "Test Accuracy 0.5\n",
      "*****Iter 19200*****\n",
      "Train Loss: 5.2053633 Test Loss: 208.19835\n",
      "Test Accuracy 0.5\n",
      "*****Iter 19300*****\n",
      "Train Loss: 5.2053633 Test Loss: 211.71169\n",
      "Test Accuracy 0.49\n",
      "*****Iter 19400*****\n",
      "Train Loss: 5.2053633 Test Loss: 187.11845\n",
      "Test Accuracy 0.56\n",
      "*****Iter 19500*****\n",
      "Train Loss: 5.2053633 Test Loss: 197.65843\n",
      "Test Accuracy 0.53\n",
      "*****Iter 19600*****\n",
      "Train Loss: 5.2053633 Test Loss: 204.68507\n",
      "Test Accuracy 0.51\n",
      "*****Iter 19700*****\n",
      "Train Loss: 5.2053633 Test Loss: 215.22498\n",
      "Test Accuracy 0.48\n",
      "*****Iter 19800*****\n",
      "Train Loss: 5.205366 Test Loss: 208.19817\n",
      "Test Accuracy 0.5\n",
      "*****Iter 19900*****\n",
      "Train Loss: 5.205374 Test Loss: 215.20813\n",
      "Test Accuracy 0.48\n",
      "*****Iter 20000*****\n",
      "Train Loss: 5.2053785 Test Loss: 222.25143\n",
      "Test Accuracy 0.46\n",
      "*****Iter 20100*****\n",
      "Train Loss: 5.2053633 Test Loss: 197.65839\n",
      "Test Accuracy 0.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Iter 20200*****\n",
      "Train Loss: 5.2053633 Test Loss: 196.06392\n",
      "Test Accuracy 0.535\n",
      "*****Iter 20300*****\n",
      "Train Loss: 5.2053633 Test Loss: 190.63174\n",
      "Test Accuracy 0.55\n",
      "*****Iter 20400*****\n",
      "Train Loss: 5.205364 Test Loss: 203.09116\n",
      "Test Accuracy 0.515\n",
      "*****Iter 20500*****\n",
      "Train Loss: 5.2053633 Test Loss: 201.1717\n",
      "Test Accuracy 0.52\n",
      "*****Iter 20600*****\n",
      "Train Loss: 5.2053633 Test Loss: 208.19841\n",
      "Test Accuracy 0.5\n",
      "*****Iter 20700*****\n",
      "Train Loss: 5.2053633 Test Loss: 211.71169\n",
      "Test Accuracy 0.49\n",
      "*****Iter 20800*****\n",
      "Train Loss: 5.2053633 Test Loss: 180.09143\n",
      "Test Accuracy 0.58\n",
      "*****Iter 20900*****\n",
      "Train Loss: 5.2053633 Test Loss: 225.7646\n",
      "Test Accuracy 0.45\n",
      "*****Iter 21000*****\n",
      "Train Loss: 5.2053633 Test Loss: 204.68512\n",
      "Test Accuracy 0.51\n",
      "*****Iter 21100*****\n",
      "Train Loss: 5.2053633 Test Loss: 190.63191\n",
      "Test Accuracy 0.55\n",
      "*****Iter 21200*****\n",
      "Train Loss: 5.2053633 Test Loss: 215.225\n",
      "Test Accuracy 0.48\n",
      "*****Iter 21300*****\n",
      "Train Loss: 5.2053633 Test Loss: 176.57874\n",
      "Test Accuracy 0.59\n",
      "*****Iter 21400*****\n",
      "Train Loss: 5.2053633 Test Loss: 215.22502\n",
      "Test Accuracy 0.48\n",
      "*****Iter 21500*****\n",
      "Train Loss: 5.2053633 Test Loss: 236.30452\n",
      "Test Accuracy 0.42\n",
      "*****Iter 21600*****\n",
      "Train Loss: 5.2053633 Test Loss: 229.27817\n",
      "Test Accuracy 0.44\n",
      "*****Iter 21700*****\n",
      "Train Loss: 5.205365 Test Loss: 211.71169\n",
      "Test Accuracy 0.49\n",
      "*****Iter 21800*****\n",
      "Train Loss: 5.2053633 Test Loss: 215.22496\n",
      "Test Accuracy 0.48\n",
      "*****Iter 21900*****\n",
      "Train Loss: 5.2053633 Test Loss: 208.1984\n",
      "Test Accuracy 0.5\n",
      "*****Iter 22000*****\n",
      "Train Loss: 5.205368 Test Loss: 218.73813\n",
      "Test Accuracy 0.47\n",
      "*****Iter 22100*****\n",
      "Train Loss: 5.2053633 Test Loss: 225.7649\n",
      "Test Accuracy 0.45\n",
      "*****Iter 22200*****\n",
      "Train Loss: 5.2053633 Test Loss: 225.76474\n",
      "Test Accuracy 0.45\n",
      "*****Iter 22300*****\n",
      "Train Loss: 5.2053633 Test Loss: 211.71169\n",
      "Test Accuracy 0.49\n",
      "*****Iter 22400*****\n",
      "Train Loss: 5.2053633 Test Loss: 239.8181\n",
      "Test Accuracy 0.41\n",
      "*****Iter 22500*****\n",
      "Train Loss: 5.2053633 Test Loss: 211.71172\n",
      "Test Accuracy 0.49\n",
      "*****Iter 22600*****\n",
      "Train Loss: 5.205364 Test Loss: 215.22499\n",
      "Test Accuracy 0.48\n",
      "*****Iter 22700*****\n",
      "Train Loss: 5.2053633 Test Loss: 217.1446\n",
      "Test Accuracy 0.475\n",
      "*****Iter 22800*****\n",
      "Train Loss: 5.2053633 Test Loss: 222.25139\n",
      "Test Accuracy 0.46\n",
      "*****Iter 22900*****\n",
      "Train Loss: 5.2053633 Test Loss: 224.1715\n",
      "Test Accuracy 0.455\n",
      "*****Iter 23000*****\n",
      "Train Loss: 5.2053633 Test Loss: 204.6851\n",
      "Test Accuracy 0.51\n",
      "*****Iter 23100*****\n",
      "Train Loss: 5.2053633 Test Loss: 218.7383\n",
      "Test Accuracy 0.47\n",
      "*****Iter 23200*****\n",
      "Train Loss: 5.2053633 Test Loss: 201.17175\n",
      "Test Accuracy 0.52\n",
      "*****Iter 23300*****\n",
      "Train Loss: 5.2053633 Test Loss: 232.79124\n",
      "Test Accuracy 0.43\n",
      "*****Iter 23400*****\n",
      "Train Loss: 5.2053633 Test Loss: 208.1984\n",
      "Test Accuracy 0.5\n",
      "*****Iter 23500*****\n",
      "Train Loss: 5.2053633 Test Loss: 225.76491\n",
      "Test Accuracy 0.45\n",
      "*****Iter 23600*****\n",
      "Train Loss: 5.2053633 Test Loss: 208.1984\n",
      "Test Accuracy 0.5\n",
      "*****Iter 23700*****\n",
      "Train Loss: 5.2053633 Test Loss: 218.73831\n",
      "Test Accuracy 0.47\n",
      "*****Iter 23800*****\n",
      "Train Loss: 5.2053633 Test Loss: 197.65851\n",
      "Test Accuracy 0.53\n",
      "*****Iter 23900*****\n",
      "Train Loss: 5.2053633 Test Loss: 232.79137\n",
      "Test Accuracy 0.43\n",
      "*****Iter 24000*****\n",
      "Train Loss: 5.2053638 Test Loss: 176.57872\n",
      "Test Accuracy 0.59\n",
      "*****Iter 24100*****\n",
      "Train Loss: 5.2053633 Test Loss: 236.30473\n",
      "Test Accuracy 0.42\n",
      "*****Iter 24200*****\n",
      "Train Loss: 5.2053633 Test Loss: 229.27815\n",
      "Test Accuracy 0.44\n",
      "*****Iter 24300*****\n",
      "Train Loss: 5.2053633 Test Loss: 204.6851\n",
      "Test Accuracy 0.51\n",
      "*****Iter 24400*****\n",
      "Train Loss: 5.2053633 Test Loss: 204.68501\n",
      "Test Accuracy 0.51\n",
      "*****Iter 24500*****\n",
      "Train Loss: 5.2053633 Test Loss: 201.17181\n",
      "Test Accuracy 0.52\n",
      "*****Iter 24600*****\n",
      "Train Loss: 5.2053633 Test Loss: 201.17181\n",
      "Test Accuracy 0.52\n",
      "*****Iter 24700*****\n",
      "Train Loss: 5.2053633 Test Loss: 204.68512\n",
      "Test Accuracy 0.51\n",
      "*****Iter 24800*****\n",
      "Train Loss: 5.2053633 Test Loss: 204.68506\n",
      "Test Accuracy 0.51\n",
      "*****Iter 24900*****\n",
      "Train Loss: 5.2053633 Test Loss: 201.17178\n",
      "Test Accuracy 0.52\n",
      "*****Iter 25000*****\n",
      "Train Loss: 5.2053633 Test Loss: 208.19841\n",
      "Test Accuracy 0.5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "results = main(num_classes=2, num_samples=1, meta_batch_size=16, random_seed=1234)\n",
    "#############################\n",
    "#### YOUR CODE GOES HERE ####\n",
    "# pass\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CS330_Homework1_Stencil.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_conda",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
